# RAG Application using LangChain, Gemini, and FAISS

This is a simple Retrieval-Augmented Generation (RAG) application that uses:

- **LangChain** for chaining data ingestion, retrieval, and generation
- **FAISS** as the in-memory vector store
- **HuggingFace Sentence Transformers** for local embeddings
- **Gemini 2.0 Flash** (via Google Generative AI) as the LLM

---

## 🔧 Tech Stack

- Python
- LangChain
- FAISS (CPU)
- Gemini (via Google Generative AI)
- HuggingFace Sentence Transformers (`all-MiniLM-L6-v2`)
- BeautifulSoup (for scraping)

---

## ⚙️ How It Works

1. **Data Ingestion**  
   Scrapes text from a Wikipedia page (`State of the Union`) using BeautifulSoup and stores the content.

2. **Text Chunking**  
   Splits raw text into manageable chunks using `RecursiveCharacterTextSplitter`.

3. **Vector Embeddings**  
   Generates vector representations of each chunk using HuggingFace embeddings.

4. **Store in FAISS**  
   Stores embeddings in a FAISS in-memory vector DB for fast similarity search.

5. **Query via Gemini**  
   Accepts user questions, retrieves relevant context from FAISS, and uses Gemini to generate concise, context-aware answers.

---

## 💬 Example Query

```python
query = "Expalin me aout State of the Union address in brief"
result = rag_chain.invoke(query)
print(result)

📦 Installation
pip install langchain langchain-community langchain-google-genai google-generativeai faiss-cpu
pip install sentence-transformers
pip install langchain-huggingface

📌 Note
Make sure to export your Google API key:
import os
os.environ["GOOGLE_API_KEY"] = "<your-api-key>"

🧠 Demo Output
Question: How is the United States supporting Ukraine economically and militarily?
Answer: [Generated by Gemini using retrieved context]

📜 License
MIT

🙌 Acknowledgements
LangChain

HuggingFace Transformers

FAISS

Gemini
